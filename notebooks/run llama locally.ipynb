{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8455f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q smolagents\n",
    "\n",
    "\n",
    "\n",
    "# There is a bug in litellm https://github.com/BerriAI/litellm/issues/10272\n",
    "#!python -m pip install -U litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3fa0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\python313.zip\n",
      "c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\DLLs\n",
      "c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\n",
      "c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\n",
      "\n",
      "C:\\Users\\dallo\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "C:\\Users\\dallo\\AppData\\Roaming\\Python\\Python313\\site-packages\\win32\n",
      "C:\\Users\\dallo\\AppData\\Roaming\\Python\\Python313\\site-packages\\win32\\lib\n",
      "C:\\Users\\dallo\\AppData\\Roaming\\Python\\Python313\\site-packages\\Pythonwin\n",
      "c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\n",
      "c:\\Users\\dallo\\workspace\n",
      "c:\\Users\\dallo\\workspace\\avrae2comicbook\n",
      "../src\n",
      "c:\\Users\\dallo\\workspace\\src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add the project root directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "add_path(os.path.abspath(os.path.join('..', '..', 'src')))\n",
    "add_path(os.path.abspath(os.path.join('..')))\n",
    "add_path(\"../src\")\n",
    "print('\\n'.join(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c95bd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the helper functions\n",
    "from utils import *\n",
    "from smolagents import CodeAgent, LiteLLMModel, InferenceClientModel, OpenAIServerModel, DuckDuckGoSearchTool\n",
    "\n",
    "gemini_api_key = get_gemini_api_key()\n",
    "model = LiteLLMModel(model_id=\"gemini/gemini-2.0-flash-lite\", api_key=gemini_api_key)\n",
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b493c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM Response:\n",
      "\n",
      "<think>\n",
      "Okay, so I need to write a Python function that generates Fibonacci numbers. Hmm, let me think about what the Fibonacci sequence is first. From what I remember, it's a sequence where each number is the sum of the two preceding ones, usually starting with 0 and 1. So like 0, 1, 1, 2, 3, 5, 8, etc.\n",
      "\n",
      "Wait, but sometimes people start with 1, 1 instead. Oh right, I think in the problem statement it just says to generate Fibonacci numbers, so maybe the initial terms are up to me as long as they follow the rule. Maybe for simplicity, let's stick with starting at 0 and 1 because that's how it's usually defined.\n",
      "\n",
      "So if I want a function that generates these numbers, perhaps in an iterative way? Because recursion can be inefficient for large n due to repeated calculations.\n",
      "\n",
      "Let me outline what the function should do. It probably needs to take a parameter indicating how many Fibonacci numbers to generate and return them as a list or something similar.\n",
      "\n",
      "Wait, actually, looking back at my initial thought process, maybe the user just wants a simple generator that can produce the sequence on demand without precomputing everything first. Oh right, because for large n, generating all previous numbers up front could be memory-intensive. So using an iterative approach with variables to keep track of the last two numbers would be efficient.\n",
      "\n",
      "Okay, so I'll create a function called fibonacci that takes no arguments? Or maybe it's better if it generates indefinitely until stopped by something like a loop condition. But perhaps for this case, we can assume the user wants to generate up to a certain count n and return the list of those numbers.\n",
      "\n",
      "Wait, but in Python, functions usually have parameters. So let me define the function with an optional parameter to specify how many terms to generate. For example:\n",
      "\n",
      "def fibonacci(n=1):\n",
      "    # code here\n",
      "\n",
      "But wait, if we don't provide any arguments, it should start generating from 0 and 1. Alternatively, maybe n defaults to a high number so that it can keep generating as needed.\n",
      "\n",
      "Alternatively, perhaps the function is meant to be an iterator or generator object which yields numbers one by one when called. Oh right! That could be useful because then each number is generated on demand without storing all previous ones in memory beyond what's necessary for computation.\n",
      "\n",
      "So using a generator approach would make sense here. Let me recall how generators work in Python. You define a function with yield statements, and it can produce values one at a time as the caller requests them.\n",
      "\n",
      "But wait, in this case, generating Fibonacci numbers could be done iteratively without even needing to use recursion or memoization. So perhaps I should write an iterative generator that yields each number step by step.\n",
      "\n",
      "Let me sketch out how that might look:\n",
      "\n",
      "def fibonacci_generator():\n",
      "    a, b = 0, 1\n",
      "    while True:\n",
      "        yield a\n",
      "        a, b = b, a + b\n",
      "\n",
      "That seems right. Each iteration produces the next Fibonacci number. So when you call next() on this generator, it returns 0, then 1, then 1, then 2, etc.\n",
      "\n",
      "But if someone wants to generate up to a certain count n, they can iterate through the generator and collect the first n items.\n",
      "\n",
      "Alternatively, maybe the function is supposed to return all Fibonacci numbers as a list. So perhaps I should create a function that takes an integer n and returns a list of the first n Fibonacci numbers.\n",
      "\n",
      "Wait, but the user's initial question was just \"write a Python function that generates Fibonacci numbers.\" It didn't specify whether it needs to be efficient for large n or how many terms to generate. Hmm.\n",
      "\n",
      "So perhaps both approaches are valid: one as an infinite generator and another as a finite list generator.\n",
      "\n",
      "But given that in programming challenges, often people use the iterative method with variables because it's more memory efficient than recursion, especially when you need only up to a certain term without storing all previous ones beyond what's necessary for computation.\n",
      "\n",
      "So let me go back. If I write an iterative function that generates each Fibonacci number one by one, yielding them as they are generated, then it can be used in cases where the user might want to generate numbers on demand or even infinitely many.\n",
      "\n",
      "But wait, if someone wants a finite list of n terms, how would that work? They could iterate through the generator and collect the first n values. Alternatively, I could write another function called fibonacci(n) which returns a list containing the first n Fibonacci numbers.\n",
      "\n",
      "Hmm, perhaps it's better to have both: one as a generator for an infinite sequence and another as a function returning up to n terms.\n",
      "\n",
      "But given that the question is just about generating Fibonacci numbers without specific parameters, maybe I should stick with writing the iterator approach because it's more flexible. The user can decide how many to generate based on their needs.\n",
      "\n",
      "Wait, but in Python, functions are first-class citizens, and generators are a good fit for this scenario where you want to produce values incrementally.\n",
      "\n",
      "So perhaps I'll write a generator function that yields each Fibonacci number one by one, starting from 0 and 1. Then, if someone wants the first n numbers, they can collect them as needed without having to store all previous terms beyond the last two, which is efficient for both memory and computation time.\n",
      "\n",
      "Alright, so putting it all together:\n",
      "\n",
      "Define a generator function called fibonacci() that yields each number in the sequence indefinitely.\n",
      "\n",
      "Then, perhaps provide another function called generate_fibonacci(n) that uses this generator to produce a list of the first n numbers.\n",
      "\n",
      "But maybe it's better not to have extra functions and just use a single generator. So let me think: if someone calls the function fibonacci(), they get an infinite generator. But for practical purposes, users might want to limit how many numbers are generated, so perhaps having another function that takes n as an argument is useful.\n",
      "\n",
      "Alternatively, since Python allows generators to be used with loops naturally, maybe it's sufficient to just have a single generator and let the user handle it.\n",
      "\n",
      "But given that in some cases people want a list of Fibonacci numbers up to a certain point, I should probably provide both: a generator for infinite sequences and functions to get n terms.\n",
      "\n",
      "Wait, perhaps the question is simply asking for any function that can generate the sequence. So maybe writing an iterative function that returns each term step by step would suffice, but as a function, not necessarily a generator.\n",
      "\n",
      "Alternatively, if someone wants to generate multiple numbers at once, it's better to collect them into a list or another iterable structure.\n",
      "\n",
      "So perhaps in code:\n",
      "\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    for _ in range(n):\n",
      "        yield a\n",
      "        a, b = b, a + b\n",
      "\n",
      "Wait, but wait: let me test this. When n is 1, it yields 0; when n=2, yields 0 and then 1.\n",
      "\n",
      "But if someone calls next(fibonacci(5)), they get 0, 1, 1, 2, 3—exactly the first five numbers.\n",
      "\n",
      "Yes, that seems correct. So this function can be used to generate a list of Fibonacci numbers up to n terms by using [x for x in fibonacci(n)] or similar.\n",
      "\n",
      "But then again, if someone wants an infinite sequence without knowing how many terms they need upfront, perhaps the generator approach is better because it doesn't require creating all elements at once. So maybe that's more efficient and scalable.\n",
      "\n",
      "So I think the most comprehensive answer would include both: a generator function for the infinite case, and another helper function to get n numbers as a list or something similar.\n",
      "\n",
      "But given time constraints, perhaps focusing on the iterative approach within a function is enough.\n",
      "\n",
      "Alternatively, if someone just wants to generate Fibonacci numbers without using generators, they can use an iterative loop that appends each number to a list until it reaches the desired count. But since in Python you can get all elements at once by calling next() n times, maybe it's better to structure the code as a generator.\n",
      "\n",
      "In summary, my plan is:\n",
      "\n",
      "1. Implement a function called fibonacci that returns a generator yielding Fibonacci numbers indefinitely.\n",
      "2. Optionally, provide another function to generate up to a certain count and return them in a list.\n",
      "3. Ensure that each step correctly calculates the next number based on the previous two.\n",
      "\n",
      "Now, writing this out: the code would have the generator function as mentioned before.\n",
      "\n",
      "Wait, but perhaps the user just wants the function to output multiple numbers at once, like generating up to n terms. In that case, it's better to return a list of those numbers.\n",
      "\n",
      "So combining both approaches:\n",
      "\n",
      "Write a function called generate_fibonacci(n) which returns a list containing the first n Fibonacci numbers.\n",
      "\n",
      "This way, users can choose whether they want an infinite generator or a finite list based on their needs.\n",
      "\n",
      "Alternatively, if someone wants all possible Fibonacci numbers (infinite), it's better to have a separate function for that. But perhaps in this case, given the initial problem statement is vague about the requirements beyond \"generates Fibonacci numbers,\" maybe just providing a simple iterative approach within a helper function is sufficient.\n",
      "\n",
      "In conclusion, after considering different approaches and ensuring correctness, I'll proceed with writing an iterative generator-based function to yield Fibonacci numbers one by one. Additionally, provide a helper function that returns n terms as a list for convenience.\n",
      "</think>\n",
      "\n",
      "To generate Fibonacci numbers in Python efficiently, we can use an iterative approach within a generator function. This allows us to produce the sequence indefinitely or up to any desired length without storing all previous values beyond the last two.\n",
      "\n",
      "Here is the code:\n",
      "\n",
      "```python\n",
      "def fibonacci_generator():\n",
      "    a, b = 0, 1\n",
      "    while True:\n",
      "        yield a\n",
      "        a, b = b, a + b\n",
      "\n",
      "def generate_fibonacci(n):\n",
      "    fib_sequence = []\n",
      "    for num in fibonacci_generator():\n",
      "        if len(fib_sequence) >= n:\n",
      "            break\n",
      "        fib_sequence.append(num)\n",
      "    return fib_sequence[:n]\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **fibonacci_generator()**: This is an infinite generator function that yields Fibonacci numbers one at a time, starting from 0 and 1. It uses two variables `a` and `b`, updating them in each iteration to compute the next number.\n",
      "\n",
      "2. **generate_fibonacci(n)**: This function uses the generator above to collect up to `n` Fibonacci numbers into a list and returns it.\n",
      "\n",
      "**Usage Examples:**\n",
      "\n",
      "- To generate the first 5 Fibonacci numbers:\n",
      "  ```python\n",
      "  fibs = generate_fibonacci(5)\n",
      "  print(fibs)  # Output: [0, 1, 1, 2, 3]\n",
      "  ```\n",
      "\n",
      "- To get an infinite sequence and print the first few values:\n",
      "  ```python\n",
      "  gen = fibonacci_generator()\n",
      "  for _ in range(10):\n",
      "      print(next(gen))  # Outputs: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34\n",
      "  ```\n",
      "\n",
      "This approach is efficient and memory-friendly, allowing users to generate Fibonacci numbers on demand.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_base = \"http://127.0.0.1:1234/v1\"\n",
    "model_id = \"local-model\"  # can be anything for LM Studio\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer lm-studio\"  # any non-empty string\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": model_id,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Write a Python function that generates Fibonacci numbers.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{api_base}/chat/completions\", headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"🧠 LLM Response:\\n\")\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"❌ Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bc41741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Say hello</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - local-model ────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSay hello\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - local-model \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">local</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-model</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Pass model as E.g. For </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huggingface'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> inference endpoints pass in `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">completion</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'huggingface/starcoder'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,..)` </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Learn more: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.litellm.ai/docs/providers</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31mlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed \u001b[0m\n",
       "\u001b[1;33mmodel\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;35mlocal\u001b[0m\u001b[1;31m-model\u001b[0m\n",
       "\u001b[1;31m Pass model as E.g. For \u001b[0m\u001b[32m'Huggingface'\u001b[0m\u001b[1;31m inference endpoints pass in `\u001b[0m\u001b[1;35mcompletion\u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;33mmodel\u001b[0m\u001b[1;31m=\u001b[0m\u001b[32m'huggingface/starcoder'\u001b[0m\u001b[1;31m,..\u001b[0m\u001b[1;31m)\u001b[0m\u001b[1;31m` \u001b[0m\n",
       "\u001b[1;31mLearn more: \u001b[0m\u001b[4;94mhttps://docs.litellm.ai/docs/providers\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.16 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.16 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\nlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=local-model\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:1266\u001b[0m, in \u001b[0;36mCodeAgent.step\u001b[1;34m(self, memory_step)\u001b[0m\n\u001b[0;32m   1265\u001b[0m additional_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrammar\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1266\u001b[0m chat_message: ChatMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<end_code>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObservation:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCalling tools:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m memory_step\u001b[38;5;241m.\u001b[39mmodel_output_message \u001b[38;5;241m=\u001b[39m chat_message\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\models.py:976\u001b[0m, in \u001b[0;36mLiteLLMModel.__call__\u001b[1;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m completion_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_completion_kwargs(\n\u001b[0;32m    964\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    965\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    974\u001b[0m )\n\u001b[1;32m--> 976\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input_token_count \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mprompt_tokens\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\utils.py:1255\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1253\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1254\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\utils.py:1133\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\main.py:3184\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3183\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   3185\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   3186\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   3187\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   3188\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   3189\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   3190\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\main.py:1002\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1002\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1010\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[0;32m   1012\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:358\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:335\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    336\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    337\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    338\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m    339\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m    340\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    341\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    342\u001b[0m         ),\n\u001b[0;32m    343\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=local-model\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m LiteLLMModel(\n\u001b[0;32m      7\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     api_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:1234\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-studio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m CodeAgent(\n\u001b[0;32m     14\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     additional_authorized_imports\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSay hello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:341\u001b[0m, in \u001b[0;36mMultiStepAgent.run\u001b[1;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, max_steps\u001b[38;5;241m=\u001b[39mmax_steps, images\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinal_answer\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:365\u001b[0m, in \u001b[0;36mMultiStepAgent._run\u001b[1;34m(self, task, max_steps, images)\u001b[0m\n\u001b[0;32m    362\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_step(task, action_step)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     action_step\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:362\u001b[0m, in \u001b[0;36mMultiStepAgent._run\u001b[1;34m(self, task, max_steps, images)\u001b[0m\n\u001b[0;32m    360\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_action_step(step_start_time, images)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 362\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:385\u001b[0m, in \u001b[0;36mMultiStepAgent._execute_step\u001b[1;34m(self, task, memory_step)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;28mstr\u001b[39m, memory_step: ActionStep) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28;01mNone\u001b[39;00m, Any]:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_rule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m--> 385\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_answer_checks:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_final_answer(final_answer)\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:1282\u001b[0m, in \u001b[0;36mCodeAgent.step\u001b[1;34m(self, memory_step)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_markdown(\n\u001b[0;32m   1285\u001b[0m     content\u001b[38;5;241m=\u001b[39mmodel_output,\n\u001b[0;32m   1286\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput message of the LLM:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1287\u001b[0m     level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mDEBUG,\n\u001b[0;32m   1288\u001b[0m )\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;66;03m# Parse\u001b[39;00m\n",
      "\u001b[1;31mAgentGenerationError\u001b[0m: Error in generating model output:\nlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=local-model\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from smolagents import CodeAgent, LiteLLMModel, InferenceClientModel, OpenAIServerModel\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"local-model\",\n",
    "    api_base=\"http://127.0.0.1:1234\",\n",
    "    api_key=\"lm-studio\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    additional_authorized_imports=[\"numpy\"]\n",
    ")\n",
    "\n",
    "response = agent.run(\"Say hello\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d38500f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAIAgent' from 'smolagents' (c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msmolagents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIAgent\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model-name\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# doesn't matter much, must be non-empty\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m      6\u001b[0m     api_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1234/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# << this tells it to use your local server\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour-key\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# LM Studio doesn't actually verify keys, but some libs require non-empty\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OpenAIAgent' from 'smolagents' (c:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from smolagents import OpenAIAgent\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"local-model-name\",  # doesn't matter much, must be non-empty\n",
    "    temperature=0.7,\n",
    "    api_base=\"http://localhost:1234/v1\",  # << this tells it to use your local server\n",
    "    api_key=\"your-key\",  # LM Studio doesn't actually verify keys, but some libs require non-empty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358b7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">say hi</span>                                                                                                          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1msay hi\u001b[0m                                                                                                          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Client Error: Payment Required for url: </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> (Request ID: </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-680fe412-5e87cb4a78f46dea4653d581;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">f33d0dd7-a172-4762-8455-b3c26dc5d5f5</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> more monthly </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">included credits.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;36m402\u001b[0m\u001b[1;31m Client Error: Payment Required for url: \u001b[0m\n",
       "\u001b[4;94mhttps://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\n",
       "\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-680fe412-5e87cb4a78f46dea4653d581;\u001b[0m\u001b[93mf33d0dd7-a172-4762-8455-b3c26dc5d5f5\u001b[0m\u001b[1;31m)\u001b[0m\n",
       "\n",
       "\u001b[1;31mYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2\u001b[0m\u001b[1;36m0x\u001b[0m\u001b[1;31m more monthly \u001b[0m\n",
       "\u001b[1;31mincluded credits.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.49 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.49 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-680fe412-5e87cb4a78f46dea4653d581;f33d0dd7-a172-4762-8455-b3c26dc5d5f5)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:1266\u001b[0m, in \u001b[0;36mCodeAgent.step\u001b[1;34m(self, memory_step)\u001b[0m\n\u001b[0;32m   1265\u001b[0m additional_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrammar\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1266\u001b[0m chat_message: ChatMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<end_code>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObservation:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCalling tools:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m memory_step\u001b[38;5;241m.\u001b[39mmodel_output_message \u001b[38;5;241m=\u001b[39m chat_message\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\models.py:1090\u001b[0m, in \u001b[0;36mInferenceClientModel.__call__\u001b[1;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m completion_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_completion_kwargs(\n\u001b[0;32m   1082\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1083\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_input_token_count \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mprompt_tokens\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:992\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[0;32m    985\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    986\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    987\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    991\u001b[0m )\n\u001b[1;32m--> 992\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-680fe412-5e87cb4a78f46dea4653d581;f33d0dd7-a172-4762-8455-b3c26dc5d5f5)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msay hi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:341\u001b[0m, in \u001b[0;36mMultiStepAgent.run\u001b[1;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, max_steps\u001b[38;5;241m=\u001b[39mmax_steps, images\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinal_answer\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:365\u001b[0m, in \u001b[0;36mMultiStepAgent._run\u001b[1;34m(self, task, max_steps, images)\u001b[0m\n\u001b[0;32m    362\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_step(task, action_step)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     action_step\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:362\u001b[0m, in \u001b[0;36mMultiStepAgent._run\u001b[1;34m(self, task, max_steps, images)\u001b[0m\n\u001b[0;32m    360\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_action_step(step_start_time, images)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 362\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:385\u001b[0m, in \u001b[0;36mMultiStepAgent._execute_step\u001b[1;34m(self, task, memory_step)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;28mstr\u001b[39m, memory_step: ActionStep) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28;01mNone\u001b[39;00m, Any]:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_rule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m--> 385\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_answer_checks:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_final_answer(final_answer)\n",
      "File \u001b[1;32mc:\\Users\\dallo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\smolagents\\agents.py:1282\u001b[0m, in \u001b[0;36mCodeAgent.step\u001b[1;34m(self, memory_step)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_markdown(\n\u001b[0;32m   1285\u001b[0m     content\u001b[38;5;241m=\u001b[39mmodel_output,\n\u001b[0;32m   1286\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput message of the LLM:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1287\u001b[0m     level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mDEBUG,\n\u001b[0;32m   1288\u001b[0m )\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;66;03m# Parse\u001b[39;00m\n",
      "\u001b[1;31mAgentGenerationError\u001b[0m: Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-680fe412-5e87cb4a78f46dea4653d581;f33d0dd7-a172-4762-8455-b3c26dc5d5f5)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "agent.run(\"say hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
